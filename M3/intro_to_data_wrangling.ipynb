{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2030cd76",
   "metadata": {},
   "source": [
    "# Understanding Data Wrangling\n",
    "\n",
    "Every project that you work on will involve an aspect of data wrangling. The time and effort spent on this step of the data analytics project is often substantially more than the “fun” task of building models with the resulting tidy data. Lots of practice with the various transformation tasks is the best way to enhance your data wrangling skills.\n",
    "\n",
    "So, what is data wrangling? I like to describe it as the \"janitorial work\" of business analytics. We want our data clean and tidy so that we can actually use it for analysis. Admittedly, the activities of data wrangling are not super-exciting, but they are almost always necessary. In essence, we are taking the input data from the original state and putting it in a format where we can perform meaningful analysis on it. You may also hear data wrangling referred to as **data manipulation**. Every project will have its own challenges and, unfortunately, there is not a set list of ordered steps applicable to them all. The end goal is to have data in a more useful format than when we started. \n",
    "\n",
    "Three common tasks involved in the data wrangling process that we will discuss include:\n",
    "- Data cleaning\n",
    "- Data transformation\n",
    "- Data enrichment\n",
    "\n",
    "There is no inherent order to any of the data wrangling tasks. In fact, the process is iterative in nature, especially as you begin trying to use the data that you have wrangled. The picture below provides a representation of the highly iterative process of trying to gain insight from data. \n",
    "\n",
    "<img src=\"images/data_process.png\" alt=\"Iterative process of finding insight from data\" width=50%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf7d57",
   "metadata": {},
   "source": [
    "## Tidy Data\n",
    "\n",
    "Our overall goal is to transform raw, messy data into **tidy** data. What is tidy data? As defined by Hadley Wickham in his paper titled [*Tidy Data*][tidy_data],\n",
    "\n",
    "> A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations,  \n",
    "> variables and types. In *tidy data*:  \n",
    "> 1. Each variable forms a column.  \n",
    "> 2. Each observation forms a row.  \n",
    "> 3. Each type of observational unit forms a table.  \n",
    "> *Messy data* is any other arrangement of the data.\n",
    "\n",
    "Using one of the examples from Wickham's paper, a typical presentation of a dataset may look like: \n",
    "\n",
    "| | treatmenta | treatmentb |\n",
    "|--|--------|---------|\n",
    "|John Smith | - | 2 |\n",
    "|Jane Doe | 16 | 11 |\n",
    "|Mary Johnson| 3 | 1 |\n",
    "\n",
    "The tidy form of this dataset would instead look like:\n",
    "\n",
    "| person | treatment | result |\n",
    "| ------ | --------- | -------|\n",
    "| John Smith | a | - |\n",
    "| Jane Doe | a | 16 |\n",
    "| Mary Johnson | a | 3 |\n",
    "| John Smith | b | 2 |\n",
    "| Jane Doe | b | 11 |\n",
    "| Mary Johnson | b | 1 |\n",
    "\n",
    "Notice that we turned the columns into rows. You may often hear this approach referred to as making \"wide\" datasets \"long\" or \"tall\". While that is certainly one way to think of the process, Wickham avoids these imprecise terms in his paper. I strongly encourage you to read the [linked paper][tidy_data].\n",
    "\n",
    "\n",
    "[tidy_data]: https://www.jstatsoft.org/article/view/v059i10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1427044",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "Data cleaning is usually the best starting point for data wrangling. Once you have the data stored as the correct data types and easy-to-reference names, many avenues will open up for exploration. You will be able easily get summary statistics, sort, and filter the data.\n",
    "\n",
    "Some of the essential data cleaning tasks that you should become fluent with include:\n",
    "- Renaming\n",
    "- Sorting and reordering\n",
    "- Data type conversion\n",
    "- Handling duplicate data\n",
    "- Addressing missing or invalid data\n",
    "- Filtering to the desired subset of data\n",
    "\n",
    "We will explore these tasks soon. For now, let's look at a quick example of renaming a column within a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea91ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70ed6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is the file superStore.xlsx -- read it in\n",
    "our_sales = pd.read_excel('./data/superStore.xlsx')\n",
    "\n",
    "# Look at the .info() on the resulting DataFrame\n",
    "our_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddd049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's change the name of the first column\n",
    "# from 'Order ID' to 'Order_ID'\n",
    "our_sales.rename(columns={'Order ID': 'Order_ID'}, inplace=True)\n",
    "our_sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9021f7e2",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## Data Transformation\n",
    "\n",
    "After some initial data cleaning, you may find that the *shape* of your data makes it difficult to perform the originally intended analysis. In **data transformation**, we are changing the structure of the dataset to help make any follow-on analyses easier. This often involves changing which data goes in the rows and the columns. Sometimes, it is as simple as transposing the rows and columns. Other times, we need more nuanced transformations. \n",
    "\n",
    "Although I cautioned before that using the language of \"wide\" versus \"long\" format is imprecise, we will use it here to illustrate what is often meant by these \"labels\". Note that each of these formats has its merits and the best choice is highly dependent on the type of analysis you are performing. One of formats may also be better suited for a particular tool that you are using. For example, [Tableau](https://www.tableau.com/), a data visualization tool, is expecting the \"long\" format. \n",
    "\n",
    "There is a data file named `data_formats.xlsx` in the data subfolder. The first worksheet, labeled `wide`, illustrates a very small dataset in a typical \"wide\" format. The second worksheet, labeled `long`, converts the same dataset into what is typically considered \"long\" format.\n",
    "\n",
    "Let's explore these two different formats of the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35801710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the \"wide\" data into a variable named wide\n",
    "wide = pd.read_excel('./data/data_formats.xlsx', 'wide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a864751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what it looks like\n",
    "wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1355d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at .info()\n",
    "wide.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b88449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .describe() to see summary statistics\n",
    "# use include='all' and datetime_is_numeric=True\n",
    "# to see the date column\n",
    "wide.describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9985538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the \"long\" data into a variable named long\n",
    "long = pd.read_excel('./data/data_formats.xlsx', 'long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78605080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at it\n",
    "long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4debf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at .info()\n",
    "long.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .describe() to see summary statistics\n",
    "# use include='all' and datetime_is_numeric=True\n",
    "# to see the date column\n",
    "long.describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fe3e80",
   "metadata": {},
   "source": [
    "The wide format is perhaps easier and better as a presentation format for human consumption and understanding than the long format. However, using the long format is expected in many visualizations software packages, including some Python visualization packages. This is especially true if you want to color lines by the name of the variable or size the markers by the values of a certain variable. (Note: I mention lines here because this data appears to be a time series and line plots are the appropriate chart to create for time series data.) If you use the plotting capabilities of `pandas`, on the other hand, it expects the **wide** format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a4298d",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\">\n",
    "\n",
    "## Data Enrichment\n",
    "\n",
    "**Data enrichment** improves the quality of the data by adding to it in some way. You will encounter data enrichment techniques in the machine learning realm where it is often called **feature engineering**. A few actions that can enhance our data using the original data include:\n",
    "\n",
    "- Adding new columns: Using functions on the data from existing columns to create new values.\n",
    "- Binning: Turning continuous data into \"buckets\".\n",
    "- Aggregating: Rolling up data and summarizing it.\n",
    "- Resampling: Aggregating time series data at specific intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77019dee",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "The following links point you to additional resources that you might find helpful in learning this material.\n",
    "\n",
    "1. [Wickham, H.. (2014). Tidy Data. *Journal of Statistical Software*,59(10), 1-23][1].\n",
    "2. [The `pandas` User Guide][2].\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "[1]: https://www.jstatsoft.org/article/view/v059i10\n",
    "[2]: https://pandas.pydata.org/docs/user_guide/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051d8e2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "-----\n",
    "\n",
    "**&copy; 2022 - Present: Matthew D. Dean, Ph.D.   \n",
    "Clinical Associate Professor of Business Analytics at William \\& Mary.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
